{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d08a10-5d6f-4620-b043-90f39345570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "# Import required modules\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "## 2. Data Preparation\n",
    "\n",
    "\n",
    "# Define data directories\n",
    "data_dir = './COVID-19_Radiography_Dataset/Dataset'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/validation'\n",
    "test_dir = data_dir + '/test'\n",
    "\n",
    "# Define data transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load datasets\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "    for x in ['train', 'valid', 'test']\n",
    "}\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                 shuffle=True, num_workers=4)\n",
    "    for x in ['train', 'valid', 'test']\n",
    "}\n",
    "\n",
    "# Get dataset sizes\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'valid', 'test']}\n",
    "print(\"Dataset sizes:\", dataset_sizes)\n",
    "\n",
    "# Get class names\n",
    "class_names = image_datasets['train'].classes\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "\n",
    "## 3. Model Architecture\n",
    "\n",
    "\n",
    "def build_classifier(num_in_features, hidden_layers, num_out_features):\n",
    "    \"\"\"\n",
    "    Build a custom classifier for the DenseNet model\n",
    "    \"\"\"\n",
    "    classifier = nn.Sequential()\n",
    "    if hidden_layers == None:\n",
    "        classifier.add_module('fc0', nn.Linear(num_in_features, 4))\n",
    "    else:\n",
    "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
    "        classifier.add_module('fc0', nn.Linear(num_in_features, hidden_layers[0]))\n",
    "        classifier.add_module('relu0', nn.ReLU())\n",
    "        classifier.add_module('drop0', nn.Dropout(.6))\n",
    "        classifier.add_module('relu1', nn.ReLU())\n",
    "        classifier.add_module('drop1', nn.Dropout(.5))\n",
    "        for i, (h1, h2) in enumerate(layer_sizes):\n",
    "            classifier.add_module(f'fc{i+1}', nn.Linear(h1, h2))\n",
    "            classifier.add_module(f'relu{i+1}', nn.ReLU())\n",
    "            classifier.add_module(f'drop{i+1}', nn.Dropout(.5))\n",
    "        classifier.add_module('output', nn.Linear(hidden_layers[-1], num_out_features))\n",
    "    return classifier\n",
    "\n",
    "# Initialize model\n",
    "model = models.densenet201(pretrained=True)\n",
    "num_in_features = 1920\n",
    "\n",
    "# Freeze parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Build and set classifier\n",
    "classifier = build_classifier(num_in_features, hidden_layers=None, num_out_features=4)\n",
    "model.classifier = classifier\n",
    "\n",
    "# Set up loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.1)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "\n",
    "## 4. Training Function\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=20):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "\n",
    "## 5. Train the Model\n",
    "\n",
    "\n",
    "# Move model to device and train\n",
    "model = model.to(device)\n",
    "model = train_model(model, criterion, optimizer, scheduler, num_epochs=20)\n",
    "\n",
    "\n",
    "## 6. Evaluation\n",
    "\n",
    "\n",
    "def evaluate_model():\n",
    "    model.eval()\n",
    "    accuracy = 0\n",
    "\n",
    "    for inputs, labels in dataloaders['test']:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        equality = (labels.data == outputs.max(1)[1])\n",
    "        accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "\n",
    "    print(f\"Test accuracy: {accuracy/len(dataloaders['test']):.3f}\")\n",
    "\n",
    "evaluate_model()\n",
    "\n",
    "\n",
    "## 7. Save and Load Model\n",
    "\n",
    "\n",
    "def save_checkpoint():\n",
    "    model.class_to_idx = image_datasets['train'].class_to_idx\n",
    "    checkpoint = {\n",
    "        'input_size': 1920,\n",
    "        'output_size': 4,\n",
    "        'epochs': epochs,\n",
    "        'batch_size': 64,\n",
    "        'model': models.densenet201(pretrained=True),\n",
    "        'classifier': classifier,\n",
    "        'scheduler': scheduler,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'state_dict': model.state_dict(),\n",
    "        'class_to_idx': model.class_to_idx\n",
    "    }\n",
    "    torch.save(checkpoint, 'model.pth')\n",
    "\n",
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = checkpoint['model']\n",
    "    model.classifier = checkpoint['classifier']\n",
    "    model.state_dict = checkpoint['state_dict']\n",
    "    model.class_to_idx = checkpoint['class_to_idx']\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    return model, checkpoint['class_to_idx']\n",
    "\n",
    "\n",
    "## 8. Inference Functions\n",
    "\n",
    "\n",
    "def process_image(image):\n",
    "    \"\"\"Process a PIL image for use in a PyTorch model\"\"\"\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return preprocess(image.convert('RGB'))\n",
    "\n",
    "def predict(image_path, model, topk=4):\n",
    "    \"\"\"Predict the class of an image using a trained deep learning model\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    img = process_image(img)\n",
    "    img = np.expand_dims(img, 0)\n",
    "    img = torch.from_numpy(img)\n",
    "    \n",
    "    model.eval()\n",
    "    inputs = Variable(img).to(device)\n",
    "    logits = model.forward(inputs)\n",
    "    \n",
    "    ps = F.softmax(logits, dim=1)\n",
    "    topk = ps.cpu().topk(topk)\n",
    "    \n",
    "    return (e.data.numpy().squeeze().tolist() for e in topk)\n",
    "\n",
    "def view_classify(img_path, prob, classes):\n",
    "    \"\"\"View an image and its predicted classes\"\"\"\n",
    "    image = Image.open(img_path)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,10), ncols=1, nrows=2)\n",
    "    disease_name = img_path.split('/')[-2]\n",
    "    ax1.set_title(disease_name)\n",
    "    ax1.imshow(image)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    y_pos = np.arange(len(prob))\n",
    "    ax2.barh(y_pos, prob, align='center')\n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(disease_classes)\n",
    "    ax2.invert_yaxis()\n",
    "    ax2.set_title('Class Probability')\n",
    "\n",
    "\n",
    "## 9. Example Usage\n",
    "\n",
    "\n",
    "# Load and process an image\n",
    "img_path = './test/COVID/example.jpg'\n",
    "probs, classes = predict(img_path, model.to(device))\n",
    "\n",
    "# Display results\n",
    "for prob, cls in zip(probs, classes):\n",
    "    print(f\"Probability of {cat_to_name[str(cls)]} is {prob*100:.2f}%\")\n",
    "\n",
    "# Visualize results\n",
    "view_classify(img_path, probs, classes)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
